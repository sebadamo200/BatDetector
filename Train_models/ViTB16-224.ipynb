{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPeMAhTVt7jaKpXV8w9szuM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQaayBUlU2Tf","executionInfo":{"status":"ok","timestamp":1748543448740,"user_tz":-120,"elapsed":28851,"user":{"displayName":"Thesis Bats","userId":"13415237238781601803"}},"outputId":"446a1612-2a86-403d-9fb0-b6200b99dec4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/LOOCV_2models_VIT/loo_temp_Anthisnes_Chateau_de_Xhos_Camera_1_HIT.zip /content\n","!unzip /content/loo_temp_Anthisnes_Chateau_de_Xhos_Camera_1_HIT.zip -d /content > /dev/null"],"metadata":{"id":"30h4I0ipU9_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import random\n","import shutil\n","\n","# Base path\n","base_path = \"/content/loo_temp_Anthisnes_Chateau_de_Xhos_Camera_1_HIT\"\n","train_dir = os.path.join(base_path, \"train\")\n","val_dir = os.path.join(base_path, \"val\")\n","\n","background_dir = os.path.join(train_dir, \"background\")\n","bats_dir = os.path.join(train_dir, \"bats\")\n","\n","val_background_dir = os.path.join(val_dir, \"background\")\n","val_bats_dir = os.path.join(val_dir, \"bats\")\n","\n","# Fixed seed for reproducibility\n","RANDOM_SEED = 42\n","random.seed(RANDOM_SEED)\n","\n","# GÃ®te names\n","chosen_gites = [\n","    'Pont_de_Bousval_Photos_2022_PHOTO',\n","    'Modave_Camera_3_toiture_PHOTO',\n","    'Bornival_PHOTO_2023CAM04',\n","    'Pont_de_Bousval_Photos_2023_PHOTO_WK6HDBOUSVAL',\n","    'Pont_de_Bousval_Photos_2023_PHOTO_2022CAM12',\n","    'Pont_de_Bousval_Photos_2023_PHOTO_2023CAM06',\n","    'Bornival_PHOTO_2023CAM03',\n","    'Chaumont_Gistoux_Camera_2',\n","    'Chaumont_Gistoux_Camera_1',\n","    'Pont_de_Bousval_Photos_2023_PHOTO_2023CAM05',\n","    # 'Anthisnes_Chateau_de_Xhos_Camera_1_HIT',\n","    'Jenneret_Camera_1_PHOTO',\n","    'Modave_Camera_plancher_PHOTO'\n","]\n","\n","# Randomly select a gÃ®te using fixed seed\n","held_out_gite = random.choice(chosen_gites)\n","print(f\"ğŸ“¦ Holding out gÃ®te for validation: {held_out_gite}\")\n","print(f\"ğŸ§ª Reproducible with seed: {RANDOM_SEED}\")\n","\n","# Create val folders\n","os.makedirs(val_background_dir, exist_ok=True)\n","os.makedirs(val_bats_dir, exist_ok=True)\n","\n","# Function to move matching files\n","def move_files_by_gite(source_dir, target_dir, gite_name):\n","    moved_count = 0\n","    for fname in sorted(os.listdir(source_dir)):  # sort to ensure order\n","        if gite_name in fname:\n","            shutil.move(os.path.join(source_dir, fname), os.path.join(target_dir, fname))\n","            moved_count += 1\n","    return moved_count\n","\n","# Move files\n","bkg_moved = move_files_by_gite(background_dir, val_background_dir, held_out_gite)\n","bats_moved = move_files_by_gite(bats_dir, val_bats_dir, held_out_gite)\n","\n","print(f\"âœ… Moved {bkg_moved} background images and {bats_moved} bat images to validation set.\")\n"],"metadata":{"id":"1vXduXksU_NU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748541957220,"user_tz":-120,"elapsed":368,"user":{"displayName":"Thesis Bats","userId":"13415237238781601803"}},"outputId":"c81fbb91-0d9e-4427-b661-a93ce59fadd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“¦ Holding out gÃ®te for validation: Jenneret_Camera_1_PHOTO\n","ğŸ§ª Reproducible with seed: 42\n","âœ… Moved 6889 background images and 1330 bat images to validation set.\n"]}]},{"cell_type":"markdown","source":["# ViT Bat Classifier (Google Colab Setup)\n","\n","This script trains and evaluates a binary image classifier to distinguish bats from background images using a pretrained Vision Transformer (`ViT`).\n","\n","## Features\n","- Mixed-precision training using `torch.amp.autocast` for performance\n","- `torch.compile()` for model acceleration (when supported)\n","- Balanced sampling and MixUp/CutMix augmentations for robust generalization\n","- Custom Focal Loss implementation to handle class imbalance\n","- Automatic threshold tuning with ROC/PR curve analysis\n","- Evaluation metrics and visualization (AUC, F1, Precision, Recall)\n","\n","## Directory Structure\n","```\n","/content/loo_temp_<dataset_name>/\n","â”œâ”€â”€ train/\n","â”‚ â”œâ”€â”€ background/\n","â”‚ â””â”€â”€ bats/\n","â”œâ”€â”€ val/\n","â”‚ â”œâ”€â”€ background/\n","â”‚ â””â”€â”€ bats/\n","â””â”€â”€ test/\n","â”œâ”€â”€ background/\n","â””â”€â”€ bats/\n","```\n","\n","\n","Each folder should contain image files with recognizable gÃ®te prefixes (e.g. `Jenneret_Camera_1_PHOTO_img123.png`).\n","\n","---\n","\n","## Key Modules\n","\n","### Configuration\n","- Defines file paths, batch size, learning rate, number of epochs, and seeds for reproducibility.\n","\n","### Transforms and Augmentation\n","- Applies resizing, normalization, and data augmentation (rotation, jitter, perspective, blur).\n","- MixUp and CutMix applied during training.\n","\n","### Data Handling\n","- Loads data using `ImageFolder` from `torchvision.datasets`.\n","- Uses `WeightedRandomSampler` to address class imbalance in the training set.\n","\n","### Model Architecture\n","- Loads `google/vit-base-patch16-224` from HuggingFace Transformers.\n","- Replaces the classifier head with `Dropout(0.5) â†’ Linear(embed_dim, 2)`.\n","- Freezes the first two-thirds of encoder layers for transfer learning.\n","\n","### Loss Function\n","- Implements Focal Loss to reduce the contribution of easy examples and focus on difficult ones.\n","\n","### Training Loop\n","- Incorporates mixed-precision (`autocast`) and gradient scaling (`GradScaler`).\n","- Uses early stopping based on macro F1-score improvements.\n","- Learning rate scheduling via cosine annealing.\n","\n","### Threshold Optimization\n","- Finds the threshold where precision â‰ˆ recall from PR curve.\n","- Saves this threshold and plots ROC for visual reference.\n","\n","### Inference\n","- Loads the best model and optimal threshold.\n","- Runs classification on new or test images and prints predictions.\n","\n","### Final Evaluation\n","- Reports macro-averaged precision, recall, F1-score, and AUC on the test set.\n","- Plots ROC curve for test predictions.\n","\n","---\n","\n","## Hardware Requirements\n","- Optimized for Google Colab with NVIDIA T4 GPU.\n","- Falls back to CPU if GPU is not available.\n","\n","---\n","\n","## Outputs\n","- `best_model/`: Directory containing the saved model from best validation F1.\n","- `val_blob.pt`: Tensor file containing validation logits and labels.\n","- `best_thr.txt`: Optimal decision threshold for inference.\n","- Inline ROC and PR plots via `matplotlib`.\n","\n","---\n","\n","## Notes\n","- Ensure your dataset is pre-organized before starting training.\n","- Adjust `NUM_EPOCHS`, `BATCH_SIZE`, or learning rate depending on your dataset size and hardware.\n","- Script is modular and can be reused for other binary classification problems with minimal changes.\n"],"metadata":{"id":"BWlxxO5LWgHD"}},{"cell_type":"code","source":["# ==========================  bat_classifier.py  ==========================\n","\"\"\"\n","ViT-based bat/background classifier\n","- Mixed-precision (torch.amp.autocast)\n","- torch.compile(), fast DataLoader, in-file FocalLoss\n","- Optimized for Colab (NVIDIA T4)\n","\"\"\"\n","\n","import os, math, random, json, numpy as np, torch\n","from PIL import Image\n","from collections import Counter\n","from tqdm.auto import tqdm\n","from torch.utils.data import DataLoader, WeightedRandomSampler\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import InterpolationMode\n","from timm.data.mixup import Mixup\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","from transformers import ViTForImageClassification\n","from sklearn.metrics import (\n","    precision_score, recall_score, f1_score,\n","    precision_recall_curve, roc_curve, auc\n",")\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# Reproducibility\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","torch.backends.cudnn.deterministic = True\n","\n","# --- Config --- #\n","DATA_DIR      = \"/content/loo_temp_Anthisnes_Chateau_de_Xhos_Camera_1_HIT\"\n","TRAIN_DIR     = os.path.join(DATA_DIR, \"train\")\n","VAL_DIR       = os.path.join(DATA_DIR, \"val\")\n","TEST_DIR      = os.path.join(DATA_DIR, \"test\")\n","INFER_DIR     = TEST_DIR\n","BATCH_SIZE    = 32\n","NUM_EPOCHS    = 10\n","LEARNING_RATE = 3e-5\n","MODEL_NAME    = \"google/vit-base-patch16-224\"\n","OUTPUT_DIR    = \"/content/vit_finetuned_bats_Anthisnes_Chateau_de_Xhos_Camera_1_HIT\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","# --- Performance settings --- #\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","torch.set_float32_matmul_precision(\"high\")\n","try:\n","    from torch.amp import autocast, GradScaler\n","    AMP_KW = dict(device_type=\"cuda\")\n","except ImportError:\n","    from torch.cuda.amp import autocast, GradScaler\n","    AMP_KW = {}\n","scaler = GradScaler()\n","\n","IMAGE_SIZE = 224\n","\n","train_albu = A.Compose([\n","    A.RandomResizedCrop(size=(IMAGE_SIZE, IMAGE_SIZE), scale=(0.5,1.0), ratio=(0.9,1.1), p=1.0),\n","    A.HorizontalFlip(p=0.5),\n","    A.Rotate(limit=15, p=0.3),\n","    A.RandomBrightnessContrast(0.2,0.2,p=0.3),\n","    A.RandomGamma((80,120), p=0.3),\n","    A.GaussNoise(std_range=(0.04, 0.20), p=0.2),\n","    A.OneOf([A.MotionBlur(5,p=1.0),A.MedianBlur(5,p=1.0),A.Blur(5,p=1.0)], p=0.2),\n","    A.CLAHE(p=0.1),\n","    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n","    ToTensorV2(),\n","])\n","val_albu = A.Compose([\n","    A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n","    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n","    ToTensorV2(),\n","])\n","\n","mixup_fn = Mixup(\n","    num_classes=2,\n","    mixup_alpha=0.2,\n","    cutmix_alpha=1.0,\n","    prob=0.5,\n","    switch_prob=0.5,\n","    mode=\"batch\",\n","    label_smoothing=0.0\n",")\n","\n","from torchvision.datasets import ImageFolder\n","\n","class AlbumentationsFolder(ImageFolder):\n","    def __init__(self, root, transform):\n","        super().__init__(root, transform=None)\n","        self.alb_transform = transform\n","\n","    def __getitem__(self, index):\n","        path, label = self.samples[index]\n","        img = np.array(Image.open(path).convert(\"RGB\"))\n","        augmented = self.alb_transform(image=img)\n","        return augmented[\"image\"], label\n","\n","# --- Dataset & sampler --- #\n","train_set = AlbumentationsFolder(TRAIN_DIR, transform=train_albu)\n","val_set   = AlbumentationsFolder(VAL_DIR,   transform=val_albu)\n","\n","targets = train_set.targets\n","class_cnt = Counter(targets)\n","cnt = torch.tensor([class_cnt[i] for i in range(len(class_cnt))], dtype=torch.float)\n","class_weights = 1. / cnt\n","class_weights /= class_weights.sum()\n","sample_weights = class_weights[targets]\n","\n","sampler = WeightedRandomSampler(sample_weights, num_samples=len(train_set)*2, replacement=True)\n","\n","num_workers = os.cpu_count() or 2\n","train_loader = DataLoader(\n","    train_set, batch_size=BATCH_SIZE, sampler=sampler,\n","    num_workers=num_workers, pin_memory=True, persistent_workers=True, prefetch_factor=4\n",")\n","val_loader = DataLoader(\n","    val_set, batch_size=BATCH_SIZE*2, shuffle=False,\n","    num_workers=num_workers, pin_memory=True, persistent_workers=True\n",")\n","# --- Focal Loss --- #\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n","        super().__init__()\n","        if isinstance(alpha, (list, tuple)):\n","            alpha = torch.tensor(alpha, dtype=torch.float32)\n","        self.alpha, self.gamma, self.reduction = alpha, gamma, reduction\n","\n","    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n","        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n","        pt = torch.exp(-ce)\n","\n","        if isinstance(self.alpha, torch.Tensor):\n","            if targets.dtype in (torch.int64, torch.int32):\n","                a = self.alpha.to(logits.device)[targets]\n","            else:\n","                a = self.alpha.mean().to(logits.device)\n","        else:\n","            a = self.alpha\n","\n","        loss = a * (1 - pt) ** self.gamma * ce\n","        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n","\n","# --- Model setup --- #\n","base_model = ViTForImageClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels=2,\n","    id2label={0: \"background\", 1: \"bats\"},\n","    label2id={\"background\": 0, \"bats\": 1},\n","    ignore_mismatched_sizes=True\n",").to(DEVICE)\n","\n","embed_dim = base_model.classifier.in_features\n","base_model.classifier = nn.Sequential(\n","    nn.Dropout(p=0.5),\n","    nn.Linear(embed_dim, 2)\n",").to(DEVICE)\n","\n","# Freeze part of the encoder\n","freeze_upto = int(len(base_model.vit.encoder.layer) * 2 / 3)\n","for i, blk in enumerate(base_model.vit.encoder.layer):\n","    if i < freeze_upto:\n","        for p in blk.parameters():\n","            p.requires_grad = False\n","\n","try:\n","    model = torch.compile(base_model)\n","except (AttributeError, RuntimeError):\n","    model = base_model\n","\n","# --- Training setup --- #\n","alpha = torch.tensor([0.05, 0.95])\n","loss_fn = FocalLoss(alpha=alpha, gamma=2.0).to(DEVICE)\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS * len(train_loader))\n","\n","# --- Training loop --- #\n","patience, best_f1, epochs_no_imp = 3, 0.0, 0\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    model.train()\n","    train_loss = 0.0\n","    for px, y in tqdm(train_loader, desc=f\"Train {epoch}/{NUM_EPOCHS}\", unit=\"batch\"):\n","        px, y = px.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n","        px, y = mixup_fn(px, y)\n","        with autocast(**AMP_KW):\n","            logits = model(pixel_values=px).logits\n","            loss = loss_fn(logits, y)\n","        optimizer.zero_grad(set_to_none=True)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        scheduler.step()\n","        train_loss += loss.item()\n","    avg_train_loss = train_loss / len(train_loader)\n","\n","    model.eval()\n","    val_logits, val_labels = [], []\n","    val_loss = 0.0\n","    with torch.no_grad():\n","        for px, y in tqdm(val_loader, desc=\"Valid\", unit=\"batch\", leave=False):\n","            px, y = px.to(DEVICE), y.to(DEVICE)\n","            with autocast(**AMP_KW):\n","                out = model(pixel_values=px).logits\n","                loss = loss_fn(out, y)\n","            val_loss += loss.item()\n","            val_logits.append(out.cpu())\n","            val_labels.append(y.cpu())\n","\n","    val_logits = torch.cat(val_logits)\n","    val_labels = torch.cat(val_labels)\n","    avg_val_loss = val_loss / len(val_loader)\n","    preds = val_logits.argmax(dim=1)\n","\n","    # Metrics\n","    f1_macro = f1_score(val_labels, preds, average=\"macro\")\n","    precision_macro = precision_score(val_labels, preds, average=\"macro\")\n","    recall_macro = recall_score(val_labels, preds, average=\"macro\")\n","\n","    print(f\"Epoch {epoch:02d} â”‚ TrainL {avg_train_loss:.4f} â”‚ ValL {avg_val_loss:.4f} â”‚ F1 {f1_macro:.4f} â”‚ P {precision_macro:.4f} â”‚ R {recall_macro:.4f}\")\n","\n","    if f1_macro > best_f1:\n","        best_f1, epochs_no_imp = f1_macro, 0\n","        model.save_pretrained(os.path.join(OUTPUT_DIR, \"best_model\"))\n","        torch.save({\"logits\": val_logits, \"labels\": val_labels}, os.path.join(OUTPUT_DIR, \"val_blob.pt\"))\n","    else:\n","        epochs_no_imp += 1\n","        if epochs_no_imp >= patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","# --- Threshold tuning --- #\n","blob = torch.load(os.path.join(OUTPUT_DIR, \"val_blob.pt\"))\n","labels = blob[\"labels\"].numpy()\n","probs  = blob[\"logits\"].softmax(1)[:, 1].cpu().numpy()\n","\n","prec, rec, thr = precision_recall_curve(labels, probs)\n","best_thr = float(thr[np.argmin(np.abs(prec - rec))])\n","with open(os.path.join(OUTPUT_DIR, \"best_thr.txt\"), \"w\") as f:\n","    f.write(str(best_thr))\n","print(f\"Optimal threshold saved: {best_thr:.4f}\")\n","\n","# --- ROC curve --- #\n","fpr, tpr, roc_thr = roc_curve(labels, probs)\n","roc_auc = auc(fpr, tpr)\n","idx_point = np.argmin(np.abs(roc_thr - best_thr))\n","plt.figure(figsize=(6,6))\n","plt.plot(fpr, tpr, label=f\"ROC (AUC = {roc_auc:.3f})\")\n","plt.scatter([fpr[idx_point]], [tpr[idx_point]], c='red', s=50, label=f\"thr={best_thr:.3f}\")\n","plt.plot([0,1], [0,1], 'k--', alpha=0.4)\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve\")\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n","\n","# --- Inference --- #\n","print(\"\\nRunning inference on crops â€¦\")\n","model = ViTForImageClassification.from_pretrained(os.path.join(OUTPUT_DIR, \"best_model\")).to(DEVICE).eval()\n","best_thr = float(open(os.path.join(OUTPUT_DIR, \"best_thr.txt\")).read())\n","\n","for fn in os.listdir(INFER_DIR):\n","    if not fn.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\")):\n","        continue\n","    img = Image.open(os.path.join(INFER_DIR, fn)).convert(\"RGB\")\n","    with torch.no_grad(), autocast(**AMP_KW):\n","        arr = np.array(img)\n","        tensor = val_albu(image=arr)[\"image\"].unsqueeze(0).to(DEVICE)\n","        logit = model(pixel_values=tensor).logits\n","    prob = logit.softmax(1)[0, 1].item()\n","    pred = \"bats\" if prob >= best_thr else \"background\"\n","    print(f\"{fn}: {pred} (Pbat={prob:.3f})\")\n","\n","# --- Final test evaluation --- #\n","print(\"\\nEvaluating on independent TEST_DIR ...\")\n","test_set = AlbumentationsFolder(TEST_DIR, transform=val_albu)\n","test_loader = DataLoader(test_set, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=os.cpu_count() or 2, pin_memory=True, persistent_workers=True)\n","\n","model.eval()\n","test_logits, test_labels = [], []\n","with torch.no_grad():\n","    for px, y in tqdm(test_loader, desc=\"Test\", unit=\"batch\", leave=False):\n","        px, y = px.to(DEVICE), y.to(DEVICE)\n","        with autocast(**AMP_KW):\n","            out = model(pixel_values=px).logits\n","        test_logits.append(out.cpu())\n","        test_labels.append(y.cpu())\n","\n","test_logits = torch.cat(test_logits)\n","test_labels = torch.cat(test_labels)\n","test_probs = test_logits.softmax(1)[:, 1]\n","test_preds = (test_probs >= best_thr).int()\n","\n","precision = precision_score(test_labels, test_preds, average=\"macro\")\n","recall    = recall_score(test_labels, test_preds, average=\"macro\")\n","f1        = f1_score(test_labels, test_preds, average=\"macro\")\n","fpr, tpr, _ = roc_curve(test_labels, test_probs)\n","roc_auc = auc(fpr, tpr)\n","\n","print(f\"\\n TEST SET PERFORMANCE:\\nPrecision: {precision:.4f} â”‚ Recall: {recall:.4f} â”‚ F1_macro: {f1:.4f} â”‚ AUC: {roc_auc:.4f}\")\n","\n","plt.figure(figsize=(6,6))\n","plt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.3f}\")\n","plt.plot([0,1], [0,1], 'k--', alpha=0.3)\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"ROC Curve - TEST set\")\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"wOZQptzXDpSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/Final_train.zip /content\n","!unzip /content/Final_train.zip -d /content > /dev/null"],"metadata":{"id":"cdV5o2soT34p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===========================  bat_classifier.py (ViT-base)  ===========================\n","\"\"\"\n","ViT-base-patch16-224 bat / background classifier\n","â€¢ Mixed-precision via torch.amp.autocast\n","â€¢ torch.compile(), fast DataLoader, in-file Focal-Loss\n","â€¢ Trained on the entire Final_train dataset\n","\"\"\"\n","\n","import os\n","import random\n","import json\n","import math\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.datasets import ImageFolder\n","\n","from PIL import Image\n","from collections import Counter\n","from tqdm.auto import tqdm\n","\n","from torch.utils.data import DataLoader, WeightedRandomSampler\n","from transformers import ViTForImageClassification\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","from sklearn.metrics import (\n","    precision_score, recall_score, f1_score,\n","    precision_recall_curve, roc_curve, auc\n",")\n","\n","import matplotlib.pyplot as plt\n","\n","from timm.data.mixup import Mixup\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark     = True\n","torch.set_float32_matmul_precision(\"high\")\n","\n","# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","DATA_DIR      = \"/content/Final_train\"\n","TRAIN_DIR     = DATA_DIR\n","OUTPUT_DIR    = \"outputs\"\n","os.makedirs(OUTPUT_DIR, exist_ok=True)\n","\n","BATCH_SIZE    = 32\n","NUM_EPOCHS    = 10\n","LEARNING_RATE = 3e-5\n","IMAGE_SIZE    = 224\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# â”€â”€ AMP Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","try:\n","    from torch.amp import autocast, GradScaler\n","    AMP_KW = dict(device_type=\"cuda\")\n","except ImportError:\n","    from torch.cuda.amp import autocast, GradScaler\n","    AMP_KW = {}\n","scaler = GradScaler()\n","\n","# â”€â”€ MixUp â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","mixup_fn = Mixup(\n","    num_classes=2,\n","    mixup_alpha=0.2,\n","    cutmix_alpha=1.0,\n","    prob=1.0,\n","    switch_prob=0.5,\n","    mode=\"batch\",\n","    label_smoothing=0.0\n",")\n","\n","# â”€â”€ Albumentations pipelines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","train_albu = A.Compose([\n","    A.RandomResizedCrop(size=(IMAGE_SIZE, IMAGE_SIZE), scale=(0.5, 1.0), ratio=(0.9, 1.1), p=1.0),\n","    A.HorizontalFlip(p=0.5),\n","    A.Rotate(limit=15, p=0.3),\n","    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n","    A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n","    A.GaussNoise(std_range=(0.04, 0.20), p=0.2),\n","    A.OneOf([\n","        A.MotionBlur(blur_limit=5, p=1.0),\n","        A.MedianBlur(blur_limit=5, p=1.0),\n","        A.Blur(blur_limit=5, p=1.0),\n","    ], p=0.2),\n","    A.CLAHE(p=0.1),\n","    A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n","    ToTensorV2(),\n","])\n","\n","# â”€â”€ Dataset wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","class AlbumentationsDataset(torch.utils.data.Dataset):\n","    def __init__(self, folder, albu_transform):\n","        self.ds   = ImageFolder(folder, transform=None)\n","        self.albu = albu_transform\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","    def __getitem__(self, idx):\n","        img, lbl = self.ds[idx]\n","        augmented = self.albu(image=np.array(img))\n","        return augmented[\"image\"], lbl\n","\n","# â”€â”€ DataLoader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","train_set = AlbumentationsDataset(TRAIN_DIR, train_albu)\n","\n","# compute classâ€balanced sampler\n","targets       = train_set.ds.targets\n","class_cnt     = Counter(targets)\n","cnt           = torch.tensor([class_cnt[i] for i in range(len(class_cnt))], dtype=torch.float)\n","class_weights = (1. / cnt)\n","class_weights /= class_weights.sum()\n","sample_weights = class_weights[targets]\n","\n","sampler = WeightedRandomSampler(\n","    sample_weights, num_samples=len(train_set), replacement=True\n",")\n","\n","num_workers = os.cpu_count() or 2\n","train_loader = DataLoader(\n","    train_set, batch_size=BATCH_SIZE, sampler=sampler,\n","    num_workers=num_workers, pin_memory=True,\n","    persistent_workers=True, prefetch_factor=4,\n","    drop_last=True,\n",")\n","\n","# â”€â”€ Focal Loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n","        super().__init__()\n","        if isinstance(alpha, (list, tuple)):\n","            alpha = torch.tensor(alpha, dtype=torch.float32)\n","        self.alpha, self.gamma, self.reduction = alpha, gamma, reduction\n","\n","    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n","        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n","        pt = torch.exp(-ce)\n","\n","        if isinstance(self.alpha, torch.Tensor):\n","            if targets.dtype in (torch.int64, torch.int32):\n","                a = self.alpha.to(logits.device)[targets]\n","            else:\n","                a = self.alpha.mean().to(logits.device)\n","        else:\n","            a = self.alpha\n","\n","        loss = a * (1 - pt) ** self.gamma * ce\n","        return loss.mean() if self.reduction == \"mean\" else loss.sum()\n","\n","# â”€â”€ Model, optimizer, scheduler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","# load pretrained ViT\n","base_model = ViTForImageClassification.from_pretrained(\n","    \"google/vit-base-patch16-224\",\n","    num_labels=2,\n","    ignore_mismatched_sizes=True\n",")\n","\n","# freeze early transformer layers\n","total_blocks = len(base_model.vit.encoder.layer)\n","freeze_upto  = int(total_blocks * 2 / 3)\n","for i, block in enumerate(base_model.vit.encoder.layer):\n","    if i < freeze_upto:\n","        for p in block.parameters():\n","            p.requires_grad = False\n","\n","# replace classification head\n","embed_dim = base_model.classifier.in_features\n","base_model.classifier = nn.Sequential(\n","    nn.Dropout(p=0.5),\n","    nn.Linear(embed_dim, 2)\n",")\n","\n","# move to device & compile\n","try:\n","    model = torch.compile(base_model.to(DEVICE))\n","except Exception:\n","    model = base_model.to(DEVICE)\n","\n","# loss, optimizer, scheduler\n","alpha     = torch.tensor([0.05, 0.95])\n","loss_fn   = FocalLoss(alpha=alpha, gamma=2.0).to(DEVICE)\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n","scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS * len(train_loader))\n","\n","# â”€â”€ Training Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    model.train()\n","    train_loss = 0.0\n","\n","    for xb, yb in tqdm(train_loader, desc=f\"Train {epoch}/{NUM_EPOCHS}\", unit=\"batch\"):\n","        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n","        xb, yb = mixup_fn(xb, yb)\n","\n","        with autocast(**AMP_KW):\n","            logits = model(xb).logits\n","            loss   = loss_fn(logits, yb)\n","\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        scheduler.step()\n","\n","        train_loss += loss.item()\n","\n","    avg_train = train_loss / len(train_loader)\n","    print(f\"Epoch {epoch:02d} â”‚ Train Loss {avg_train:.4f}\")\n","\n","    # save checkpoint each epoch\n","    torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f\"model_epoch{epoch}.pth\"))\n","\n","# â”€â”€ Save final model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n","torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"best_model.pth\"))\n","print(\"Training complete. Model saved to\", OUTPUT_DIR)\n"],"metadata":{"id":"d7hY3508WYLR"},"execution_count":null,"outputs":[]}]}